{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for all of us\n",
    "\n",
    "Choose a dataset that you want to use. \n",
    "\n",
    "You may do whatever steps you think necessary for building the best classifier.\n",
    "\n",
    "Take the data you chose and do whatever massaging you think is necessary: standardizing, scaling, feature engineering/ transforming, feature selection, etc.  \n",
    "\n",
    "Build a classifier however you see fit. You may want build one and tweak the paramters manually or use some sort of grid search to look through all possible parameters. \n",
    "\n",
    "Remember: The same model built on the massaged data may perform better than if the data was untouched. It may be more conveniant to chose a standared massaging pipeline and tweak a model to that data.\n",
    "\n",
    "After you massage your data, follow these steps:\n",
    "\n",
    "if you want to balance your target (which you should) follow along these lines:\n",
    "\n",
    "### build your features data and target data\n",
    "\n",
    "- X = df.drop(columns = \"whatever your target name is\")\n",
    "- y = df\"whatever your target name is\"\n",
    "\n",
    "### split the data into training and testing\n",
    "\n",
    "- X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2019)\n",
    "\n",
    "### create the oversampled data to train on \n",
    "\n",
    "- oversampler = SMOTE(random_state = 2019)\n",
    "- X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "### Put the oversampled data back into a dataframe\n",
    "\n",
    "- X_train_oversampled = pd.DataFrame(X_train_oversampled, columns = X_train.columns)\n",
    "- y_train_oversampled = pd.Series(y_train_oversampled)\n",
    "\n",
    "### Build your classifier here. As an example:\n",
    "\n",
    "- xgb_clf = xgb.XGBClassifier(max_depth=5, n_estimators=100, colsample_bytree=0.3, learning_rate=0.1, n_jobs=-1)\n",
    "\n",
    " \n",
    "### Fit to the oversampled data; this will train the classifier on the oversampled data\n",
    "\n",
    "- xgb_clf.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "### Use 5-fold cross validation to see how well the classfier you built is doing on test data. \n",
    "Some points: you have to substitute your classifer name in the cross_val_score function \n",
    "\n",
    "- kfold = KFold(n_splits=5, random_state=2019)\n",
    "- results = cross_val_score(xgb_clf, X_test, y_test, cv=kfold, scoring = 'f1')\n",
    "\n",
    "\n",
    "## It may be best to keep all of your models you built; have a log of them to see their scores and keep a record of your process of building your data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from Modules import *\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, f1_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from time import time\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import RFE, SelectKBest\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read in the full sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEX_Female</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEX_Male</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDUCATION_Graduate School</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDUCATION_Other</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDUCATION_University</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARRIAGE_Married</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARRIAGE_Non-married</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL1</th>\n",
       "      <td>0.195650</td>\n",
       "      <td>0.022350</td>\n",
       "      <td>0.308011</td>\n",
       "      <td>0.899800</td>\n",
       "      <td>0.132340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL2</th>\n",
       "      <td>0.120650</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.139189</td>\n",
       "      <td>0.924280</td>\n",
       "      <td>-0.620220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL3</th>\n",
       "      <td>0.034450</td>\n",
       "      <td>0.014017</td>\n",
       "      <td>0.139544</td>\n",
       "      <td>0.961820</td>\n",
       "      <td>0.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.148122</td>\n",
       "      <td>0.544280</td>\n",
       "      <td>0.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028792</td>\n",
       "      <td>0.154978</td>\n",
       "      <td>0.557800</td>\n",
       "      <td>0.369140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010508</td>\n",
       "      <td>0.117211</td>\n",
       "      <td>0.570940</td>\n",
       "      <td>0.369040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_1_INDICATOR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_2_INDICATOR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_3_INDICATOR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_4_INDICATOR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_5_INDICATOR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051917</td>\n",
       "      <td>0.042562</td>\n",
       "      <td>0.232099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_2</th>\n",
       "      <td>0.222115</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.106937</td>\n",
       "      <td>0.041859</td>\n",
       "      <td>6.469312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372856</td>\n",
       "      <td>0.073752</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.279057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.305623</td>\n",
       "      <td>0.069779</td>\n",
       "      <td>0.038850</td>\n",
       "      <td>0.429799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066899</td>\n",
       "      <td>0.036914</td>\n",
       "      <td>0.035987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.613309</td>\n",
       "      <td>0.321564</td>\n",
       "      <td>0.033844</td>\n",
       "      <td>0.035492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_1_INDICATOR</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_2_INDICATOR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_3_INDICATOR</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_4_INDICATOR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_5_INDICATOR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_1_Other</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_1_more_than_two_month_late</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_1_on_time</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_1_one_month_late</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_2_Other</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_2_more_than_two_month_late</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_2_on_time</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_2_one_month_late</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_3_Other</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_3_more_than_two_month_late</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_3_on_time</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_3_one_month_late</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_4_Other</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_4_more_than_two_month_late</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_4_on_time</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_4_one_month_late</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_5_Other</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_5_more_than_two_month_late</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_5_on_time</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_6_Other</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_6_more_than_two_month_late</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_6_on_time</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        0          1          2          3  \\\n",
       "AGE                             24.000000  26.000000  34.000000  37.000000   \n",
       "Y                                1.000000   1.000000   0.000000   0.000000   \n",
       "SEX_Female                       1.000000   1.000000   1.000000   1.000000   \n",
       "SEX_Male                         0.000000   0.000000   0.000000   0.000000   \n",
       "EDUCATION_Graduate School        0.000000   0.000000   0.000000   0.000000   \n",
       "EDUCATION_Other                  0.000000   0.000000   0.000000   0.000000   \n",
       "EDUCATION_University             1.000000   1.000000   1.000000   1.000000   \n",
       "MARRIAGE_Married                 1.000000   0.000000   0.000000   1.000000   \n",
       "MARRIAGE_Non-married             0.000000   1.000000   1.000000   0.000000   \n",
       "PERCENT_OF_LIMIT_BAL1            0.195650   0.022350   0.308011   0.899800   \n",
       "PERCENT_OF_LIMIT_BAL2            0.120650   0.006042   0.139189   0.924280   \n",
       "PERCENT_OF_LIMIT_BAL3            0.034450   0.014017   0.139544   0.961820   \n",
       "PERCENT_OF_LIMIT_BAL4            0.000000   0.018933   0.148122   0.544280   \n",
       "PERCENT_OF_LIMIT_BAL5            0.000000   0.028792   0.154978   0.557800   \n",
       "PERCENT_OF_LIMIT_BAL6            0.000000   0.010508   0.117211   0.570940   \n",
       "PERCENT_OF_LIMIT_1_INDICATOR     0.000000   0.000000   0.000000   1.000000   \n",
       "PERCENT_OF_LIMIT_2_INDICATOR     0.000000   0.800000   0.800000   0.800000   \n",
       "PERCENT_OF_LIMIT_3_INDICATOR     0.000000   0.600000   0.600000   0.000000   \n",
       "PERCENT_OF_LIMIT_4_INDICATOR     0.000000   0.400000   0.400000   0.400000   \n",
       "PERCENT_OF_LIMIT_5_INDICATOR     0.000000   0.000000   0.000000   0.200000   \n",
       "PAY_RATIO_1                      0.000000   0.000000   0.051917   0.042562   \n",
       "PAY_RATIO_2                      0.222115   0.579710   0.106937   0.041859   \n",
       "PAY_RATIO_3                      0.000000   0.372856   0.073752   0.024345   \n",
       "PAY_RATIO_4                      1.000000   0.305623   0.069779   0.038850   \n",
       "PAY_RATIO_5                      1.000000   0.000000   0.066899   0.036914   \n",
       "PAY_RATIO_6                      1.000000   0.613309   0.321564   0.033844   \n",
       "PAY_RATIO_1_INDICATOR            1.000000   1.000000   1.000000   0.000000   \n",
       "PAY_RATIO_2_INDICATOR            0.000000   0.000000   0.000000   0.000000   \n",
       "PAY_RATIO_3_INDICATOR            0.600000   0.000000   0.000000   0.600000   \n",
       "PAY_RATIO_4_INDICATOR            0.000000   0.000000   0.000000   0.000000   \n",
       "PAY_RATIO_5_INDICATOR            0.000000   0.200000   0.200000   0.000000   \n",
       "PAY_1_Other                      0.000000   0.000000   1.000000   1.000000   \n",
       "PAY_1_more_than_two_month_late   1.000000   0.000000   0.000000   0.000000   \n",
       "PAY_1_on_time                    0.000000   1.000000   0.000000   0.000000   \n",
       "PAY_1_one_month_late             0.000000   0.000000   0.000000   0.000000   \n",
       "PAY_2_Other                      0.000000   0.000000   1.000000   1.000000   \n",
       "PAY_2_more_than_two_month_late   1.000000   1.000000   0.000000   0.000000   \n",
       "PAY_2_on_time                    0.000000   0.000000   0.000000   0.000000   \n",
       "PAY_2_one_month_late             0.000000   0.000000   0.000000   0.000000   \n",
       "PAY_3_Other                      0.000000   1.000000   1.000000   1.000000   \n",
       "PAY_3_more_than_two_month_late   0.000000   0.000000   0.000000   0.000000   \n",
       "PAY_3_on_time                    1.000000   0.000000   0.000000   0.000000   \n",
       "PAY_3_one_month_late             0.000000   0.000000   0.000000   0.000000   \n",
       "PAY_4_Other                      0.000000   1.000000   1.000000   1.000000   \n",
       "PAY_4_more_than_two_month_late   0.000000   0.000000   0.000000   0.000000   \n",
       "PAY_4_on_time                    1.000000   0.000000   0.000000   0.000000   \n",
       "PAY_4_one_month_late             0.000000   0.000000   0.000000   0.000000   \n",
       "PAY_5_Other                      1.000000   1.000000   1.000000   1.000000   \n",
       "PAY_5_more_than_two_month_late   0.000000   0.000000   0.000000   0.000000   \n",
       "PAY_5_on_time                    0.000000   0.000000   0.000000   0.000000   \n",
       "PAY_6_Other                      1.000000   0.000000   1.000000   1.000000   \n",
       "PAY_6_more_than_two_month_late   0.000000   1.000000   0.000000   0.000000   \n",
       "PAY_6_on_time                    0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "                                        4  \n",
       "AGE                             57.000000  \n",
       "Y                                0.000000  \n",
       "SEX_Female                       0.000000  \n",
       "SEX_Male                         1.000000  \n",
       "EDUCATION_Graduate School        0.000000  \n",
       "EDUCATION_Other                  0.000000  \n",
       "EDUCATION_University             1.000000  \n",
       "MARRIAGE_Married                 1.000000  \n",
       "MARRIAGE_Non-married             0.000000  \n",
       "PERCENT_OF_LIMIT_BAL1            0.132340  \n",
       "PERCENT_OF_LIMIT_BAL2           -0.620220  \n",
       "PERCENT_OF_LIMIT_BAL3            0.516700  \n",
       "PERCENT_OF_LIMIT_BAL4            0.238800  \n",
       "PERCENT_OF_LIMIT_BAL5            0.369140  \n",
       "PERCENT_OF_LIMIT_BAL6            0.369040  \n",
       "PERCENT_OF_LIMIT_1_INDICATOR     0.000000  \n",
       "PERCENT_OF_LIMIT_2_INDICATOR     0.800000  \n",
       "PERCENT_OF_LIMIT_3_INDICATOR     0.000000  \n",
       "PERCENT_OF_LIMIT_4_INDICATOR     0.400000  \n",
       "PERCENT_OF_LIMIT_5_INDICATOR     0.000000  \n",
       "PAY_RATIO_1                      0.232099  \n",
       "PAY_RATIO_2                      6.469312  \n",
       "PAY_RATIO_3                      0.279057  \n",
       "PAY_RATIO_4                      0.429799  \n",
       "PAY_RATIO_5                      0.035987  \n",
       "PAY_RATIO_6                      0.035492  \n",
       "PAY_RATIO_1_INDICATOR            1.000000  \n",
       "PAY_RATIO_2_INDICATOR            0.000000  \n",
       "PAY_RATIO_3_INDICATOR            0.600000  \n",
       "PAY_RATIO_4_INDICATOR            0.000000  \n",
       "PAY_RATIO_5_INDICATOR            0.000000  \n",
       "PAY_1_Other                      0.000000  \n",
       "PAY_1_more_than_two_month_late   0.000000  \n",
       "PAY_1_on_time                    1.000000  \n",
       "PAY_1_one_month_late             0.000000  \n",
       "PAY_2_Other                      1.000000  \n",
       "PAY_2_more_than_two_month_late   0.000000  \n",
       "PAY_2_on_time                    0.000000  \n",
       "PAY_2_one_month_late             0.000000  \n",
       "PAY_3_Other                      0.000000  \n",
       "PAY_3_more_than_two_month_late   0.000000  \n",
       "PAY_3_on_time                    1.000000  \n",
       "PAY_3_one_month_late             0.000000  \n",
       "PAY_4_Other                      1.000000  \n",
       "PAY_4_more_than_two_month_late   0.000000  \n",
       "PAY_4_on_time                    0.000000  \n",
       "PAY_4_one_month_late             0.000000  \n",
       "PAY_5_Other                      1.000000  \n",
       "PAY_5_more_than_two_month_late   0.000000  \n",
       "PAY_5_on_time                    0.000000  \n",
       "PAY_6_Other                      1.000000  \n",
       "PAY_6_more_than_two_month_late   0.000000  \n",
       "PAY_6_on_time                    0.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the data; data formated weird with extra column, \n",
    "#use only data we need\n",
    "data = pd.read_csv('Data/Final_trimmed_sequential_data.csv')\n",
    "df = data.iloc[:, 1:]\n",
    "y = df['Y']\n",
    "\n",
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross-validation results: 0.7822333333333333\n"
     ]
    }
   ],
   "source": [
    "#make pipeline\n",
    "\n",
    "\n",
    "\n",
    "#split into training and testing\n",
    "Train_data, test_data = train_test_split(df, test_size = 0.2, random_state = 2019)\n",
    "\n",
    "#set the target and predictor labels\n",
    "target = 'Y'\n",
    "predictors = [x for x in df.columns if x not in [target] ]\n",
    "\n",
    "#build the classifier\n",
    "clf = LogisticRegression()\n",
    "\n",
    "#set up the imbalanced data handling procedure\n",
    "oversampler = SMOTE(random_state = 2019)\n",
    "\n",
    "#creat the pipeline\n",
    "pipeline = Pipeline([('smote', oversampler),('clf', clf)])\n",
    "\n",
    "#cross validate results\n",
    "kfold = KFold(n_splits=5, random_state=2019)\n",
    "results = cross_val_score(pipeline, df[predictors], df[target], cv=kfold, scoring = 'accuracy')\n",
    "print(f\"5-fold cross-validation results: {np.mean(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.782 \n",
      "\n",
      "AUC:0.705 \n",
      "\n",
      "F1:0.529 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86      4710\n",
      "           1       0.49      0.57      0.53      1290\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6000\n",
      "   macro avg       0.69      0.70      0.69      6000\n",
      "weighted avg       0.79      0.78      0.79      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#make helper function\n",
    "\n",
    "def evaluate_clf(data, model, imbalance='SMOTE', params=None, show_accuracy=True, show_auc=True, show_f1=True):\n",
    "    \n",
    "\n",
    "    #split into training and testing\n",
    "    Train_data, test_data = train_test_split(data, test_size = 0.2, random_state = 2019)\n",
    "\n",
    "    #set the target and predictor labels\n",
    "    target = 'Y'\n",
    "    predictors = [x for x in data.columns if x not in [target] ]\n",
    "\n",
    "    #build the classifier\n",
    "    clf = model\n",
    "\n",
    "    #set up the imbalanced data handling procedure\n",
    "    if imbalance=='SMOTE':\n",
    "        oversampler = SMOTE(random_state = 2019)\n",
    "    else:\n",
    "        oversampler = RandomOverSampler(random_state=2019)\n",
    "    #creat the pipeline\n",
    "    pipeline = Pipeline([('smote', oversampler),('clf', clf)])\n",
    "    pipeline.fit(Train_data[predictors], Train_data[target])\n",
    "    y_pred = pipeline.predict(test_data[predictors])\n",
    "    \n",
    "    if show_accuracy:\n",
    "        print (\"Accuracy:{0:.3f}\".format(accuracy_score(test_data[target],y_pred)),\"\\n\")\n",
    "        \n",
    "    if show_auc:\n",
    "        print (\"AUC:{0:.3f}\".format(roc_auc_score(test_data[target],y_pred)),\"\\n\")\n",
    "        \n",
    "    if show_f1:\n",
    "        print (\"F1:{0:.3f}\".format(f1_score(test_data[target],y_pred)),\"\\n\")\n",
    "    \n",
    "    print(classification_report(test_data[target], y_pred))\n",
    "\n",
    "evaluate_clf(df, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for pipeline\n",
    "def evaluate_pipeline(data, pipeline, params=None, show_accuracy=True, show_auc=True, show_f1=True):\n",
    "    \n",
    "\n",
    "    #split into training and testing\n",
    "    Train_data, test_data = train_test_split(data, test_size = 0.2, random_state = 2019)\n",
    "\n",
    "    #set the target and predictor labels\n",
    "    target = 'Y'\n",
    "    predictors = [x for x in data.columns if x not in [target] ]\n",
    "    \n",
    "    #fit the pipeline\n",
    "    pipe = pipeline\n",
    "    pipe.fit(Train_data[predictors], Train_data[target])\n",
    "    y_pred = pipeline.predict(test_data[predictors])\n",
    "    \n",
    "    if show_accuracy:\n",
    "        print (\"Accuracy:{0:.3f}\".format(accuracy_score(test_data[target],y_pred)),\"\\n\")\n",
    "        \n",
    "    if show_auc:\n",
    "        print (\"AUC:{0:.3f}\".format(roc_auc_score(test_data[target],y_pred)),\"\\n\")\n",
    "        \n",
    "    if show_f1:\n",
    "        print (\"F1:{0:.3f}\".format(f1_score(test_data[target],y_pred)),\"\\n\")\n",
    "    \n",
    "    print(classification_report(test_data[target], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create RandomSearchCV helper function\n",
    "def evaluate_random_grid(data, model, grid_params, score, n_iter=10,\n",
    "                         cv = 5):\n",
    "\n",
    "    #split into training and testing\n",
    "    Train_data, test_data = train_test_split(data, test_size = 0.2, random_state = 2019)\n",
    "    \n",
    "    #set the target and predictor labels\n",
    "    target = 'Y'\n",
    "    predictors = [x for x in data.columns if x not in [target] ]\n",
    "\n",
    "    #build the classifier\n",
    "    grid = RandomizedSearchCV(estimator = model, param_distributions = grid_params, n_iter=n_iter, \n",
    "                            cv = cv, n_jobs=-1, scoring = score)\n",
    "    \n",
    "    #fit the random search\n",
    "    start = time()\n",
    "    cv_results = grid.fit(Train_data[predictors], Train_data[target])\n",
    "    print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter))\n",
    "    print()\n",
    "    \n",
    "    print(f\"RandomizedSearchCV grid model {model} with parameters {cv_results.best_params_} had a best score of {cv_results.best_score_}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create GridSearchCV helper\n",
    "def evaluate_gridsearch(data, model, grid_params, score,\n",
    "                         cv = 5):\n",
    "\n",
    "    #split into training and testing\n",
    "    Train_data, test_data = train_test_split(data, test_size = 0.2, random_state = 2019)\n",
    "    \n",
    "    #set the target and predictor labels\n",
    "    target = 'Y'\n",
    "    predictors = [x for x in data.columns if x not in [target] ]\n",
    "\n",
    "    #build the classifier\n",
    "    grid = GridSearchCV(estimator = model, param_grid = grid_params, \n",
    "                            cv = cv, n_jobs=-1, scoring = score)\n",
    "    \n",
    "    #fit the random search\n",
    "    start = time()\n",
    "    cv_results = grid.fit(Train_data[predictors], Train_data[target])\n",
    "    print(\"GridSearchCV took %.2f seconds\" % ((time() - start)))\n",
    "    print()\n",
    "    \n",
    "    print(f\"GridSearchCV grid model {model} with parameters {cv_results.best_params_}  had a best score of {cv_results.best_score_}\")\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-96861cf389a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mevaluate_random_grid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-bb482b5b9220>\u001b[0m in \u001b[0;36mevaluate_random_grid\u001b[1;34m(data, model, grid_params, score, n_iter, cv)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#fit the random search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mcv_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n\u001b[0;32m     20\u001b[0m       \" parameter settings.\" % ((time() - start), n_iter))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1513\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1514\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1515\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test procudure\n",
    "clf = LogisticRegression()\n",
    "pipe = Pipeline([\n",
    "    ('smote', SMOTE(random_state=2019)),\n",
    "    ('clf', clf)])\n",
    "param_grid = {\n",
    "    'clf__C': [1,5],\n",
    "    'clf__penalty': ['l1', 'l2']\n",
    "          }\n",
    "\n",
    "\n",
    "\n",
    "evaluate_random_grid(df, pipe, grid_params=param_grid, n_iter=4, score = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 71.78 seconds\n",
      "\n",
      "GridSearchCV grid model Pipeline(memory=None,\n",
      "     steps=[('smote', SMOTE(k_neighbors=5, kind='deprecated', m_neighbors='deprecated', n_jobs=1,\n",
      "   out_step='deprecated', random_state=2019, ratio=None,\n",
      "   sampling_strategy='auto', svm_estimator='deprecated')), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))]) with parameters {'clf__C': 1, 'clf__penalty': 'l1'}  had a best score of 0.779125\n"
     ]
    }
   ],
   "source": [
    "param_grid2 = {\n",
    "    'clf__C': [1,2,3],\n",
    "    'clf__penalty': ['l1']\n",
    "}\n",
    "\n",
    "evaluate_gridsearch(df, pipe, grid_params=param_grid2, score='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.782 \n",
      "\n",
      "AUC:0.705 \n",
      "\n",
      "F1:0.529 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86      4710\n",
      "           1       0.49      0.57      0.53      1290\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6000\n",
      "   macro avg       0.69      0.70      0.69      6000\n",
      "weighted avg       0.79      0.78      0.79      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1, penalty='l1')\n",
    "pipe = Pipeline([\n",
    "    ('smote', SMOTE(random_state=2019)),\n",
    "    ('clf', clf)])\n",
    "\n",
    "evaluate_pipeline(df, pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.782 \n",
      "\n",
      "AUC:0.705 \n",
      "\n",
      "F1:0.529 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86      4710\n",
      "           1       0.49      0.57      0.53      1290\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6000\n",
      "   macro avg       0.69      0.70      0.69      6000\n",
      "weighted avg       0.79      0.78      0.79      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(df, LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.781 \n",
      "\n",
      "AUC:0.705 \n",
      "\n",
      "F1:0.529 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86      4710\n",
      "           1       0.49      0.57      0.53      1290\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6000\n",
      "   macro avg       0.69      0.70      0.69      6000\n",
      "weighted avg       0.79      0.78      0.79      6000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-khickey/.local/lib/python3.6/site-packages/imblearn/pipeline.py:349: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "#testing a pipeline object in the helper functions\n",
    "clf = LogisticRegression()\n",
    "pipe = Pipeline([\n",
    "    ('smote', SMOTE(random_state=2019)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', clf)])\n",
    "\n",
    "evaluate_pipeline(data=df, pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:271: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 28.00 seconds for 10 candidates parameter settings.\n",
      "\n",
      "RandomizedSearchCV grid model Pipeline(memory=None,\n",
      "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))]) with parameters {'clf__penalty': 'l1', 'clf__C': 0.5} had a best score of 0.6576356788221195\n"
     ]
    }
   ],
   "source": [
    "#testing same code but with a random search\n",
    "clf = LogisticRegression()\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', clf)])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__C': [0.5,1, 1.5],\n",
    "    'clf__penalty': ['l1', 'l2']\n",
    "             }\n",
    "evaluate_random_grid(df, model=pipe, cv=5, grid_params=param_grid,score='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# This dataset is way to high-dimensional. Better do PCA:\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Maybe some original features where good, too?\n",
    "selection = SelectKBest(k=1)\n",
    "\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "# Model:\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Do grid search over k, n_components and C:\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n",
    "param_grid = dict(features__pca__n_components=[1, 2, 3],\n",
    "                  features__univ_select__k=[1, 2],\n",
    "                  svm__C=[0.1, 1, 10])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10, n_jobs=-1 )\n",
    "grid_search.fit(df[predictors], y)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.801 \n",
      "\n",
      "AUC:0.642 \n",
      "\n",
      "F1:0.440 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88      4710\n",
      "           1       0.56      0.36      0.44      1290\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6000\n",
      "   macro avg       0.70      0.64      0.66      6000\n",
      "weighted avg       0.78      0.80      0.78      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# attempting with a random forest\n",
    "\n",
    "clfRF = RandomForestClassifier()\n",
    "evaluate_clf(data=df, model=clfRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.6963333333333334\n"
     ]
    }
   ],
   "source": [
    "#testing the seperate fit to training, and predicting\n",
    "\n",
    "clf4 = LogisticRegression()\n",
    "\n",
    "pipeline4 = Pipeline([('smote', oversampler), ('clf', clf4)])\n",
    "pipeline4.fit(Train_data[predictors], Train_data[target])\n",
    "\n",
    "y_pred = pipeline4.predict(test_data[predictors])\n",
    "\n",
    "print(f\"accuracy score: {accuracy_score(test_data[target], y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross-validation results: 0.7969999999999999\n"
     ]
    }
   ],
   "source": [
    "#make pipeline with randomforest\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#split into training and testing\n",
    "Train_data, test_data = train_test_split(df, test_size = 0.2, random_state = 2019)\n",
    "\n",
    "target = 'Y'\n",
    "predictors = [x for x in Train_data.columns if x not in [target] ]\n",
    "\n",
    "clfRF = RandomForestClassifier()\n",
    "\n",
    "oversampler = SMOTE(random_state = 2019)\n",
    "pipeline = Pipeline([('smote', oversampler),('clf', clfRF)])\n",
    "\n",
    "#cross validate results\n",
    "kfold = KFold(n_splits=5, random_state=2019)\n",
    "results = cross_val_score(pipeline, Train_data[predictors], Train_data[target], cv=kfold, scoring = 'accuracy')\n",
    "print(f\"5-fold cross-validation results: {np.mean(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross-validation results: 0.7866666666666667\n"
     ]
    }
   ],
   "source": [
    "#compare the XGBoost models\n",
    "\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = joblib.load('Models/xgboost3.dat')\n",
    "#pipeline5 =  Pipeline([('smote', oversampler),('clf', model)])\n",
    "kfold = KFold(n_splits=5, random_state=2019)\n",
    "results = cross_val_score(model, test_data[predictors], test_data[target], cv=kfold, scoring = 'accuracy')\n",
    "print(f\"5-fold cross-validation results: {np.mean(results)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

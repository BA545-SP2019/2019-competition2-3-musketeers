{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for all of us\n",
    "\n",
    "Choose a dataset that you want to use. \n",
    "\n",
    "You may do whatever steps you think necessary for building the best classifier.\n",
    "\n",
    "Take the data you chose and do whatever massaging you think is necessary: standardizing, scaling, feature engineering/ transforming, feature selection, etc.  \n",
    "\n",
    "Build a classifier however you see fit. You may want build one and tweak the paramters manually or use some sort of grid search to look through all possible parameters. \n",
    "\n",
    "Remember: The same model built on the massaged data may perform better than if the data was untouched. It may be more conveniant to chose a standared massaging pipeline and tweak a model to that data.\n",
    "\n",
    "After you massage your data, follow these steps:\n",
    "\n",
    "if you want to balance your target (which you should) follow along these lines:\n",
    "\n",
    "### build your features data and target data\n",
    "\n",
    "- X = df.drop(columns = \"whatever your target name is\")\n",
    "- y = df\"whatever your target name is\"\n",
    "\n",
    "### split the data into training and testing\n",
    "\n",
    "- X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2019)\n",
    "\n",
    "### create the oversampled data to train on \n",
    "\n",
    "- oversampler = SMOTE(random_state = 2019)\n",
    "- X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "### Put the oversampled data back into a dataframe\n",
    "\n",
    "- X_train_oversampled = pd.DataFrame(X_train_oversampled, columns = X_train.columns)\n",
    "- y_train_oversampled = pd.Series(y_train_oversampled)\n",
    "\n",
    "### Build your classifier here. As an example:\n",
    "\n",
    "- xgb_clf = xgb.XGBClassifier(max_depth=5, n_estimators=100, colsample_bytree=0.3, learning_rate=0.1, n_jobs=-1)\n",
    "\n",
    " \n",
    "### Fit to the oversampled data; this will train the classifier on the oversampled data\n",
    "\n",
    "- xgb_clf.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "### Use 5-fold cross validation to see how well the classfier you built is doing on test data. \n",
    "Some points: you have to substitute your classifer name in the cross_val_score function \n",
    "\n",
    "- kfold = KFold(n_splits=5, random_state=2019)\n",
    "- results = cross_val_score(xgb_clf, X_test, y_test, cv=kfold, scoring = 'f1')\n",
    "\n",
    "\n",
    "## It may be best to keep all of your models you built; have a log of them to see their scores and keep a record of your process of building your data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from Modules import *\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, f1_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from time import time\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import RFE, SelectKBest\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read in the full sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_2</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_4</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_5</th>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_6</th>\n",
       "      <td>-2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEX_Female</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEX_Male</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDUCATION_Graduate School</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDUCATION_Other</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDUCATION_University</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARRIAGE_Married</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARRIAGE_Non-married</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL1</th>\n",
       "      <td>0.195650</td>\n",
       "      <td>0.022350</td>\n",
       "      <td>0.308011</td>\n",
       "      <td>0.899800</td>\n",
       "      <td>0.132340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL2</th>\n",
       "      <td>0.120650</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.139189</td>\n",
       "      <td>0.924280</td>\n",
       "      <td>-0.620220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL3</th>\n",
       "      <td>0.034450</td>\n",
       "      <td>0.014017</td>\n",
       "      <td>0.139544</td>\n",
       "      <td>0.961820</td>\n",
       "      <td>0.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.148122</td>\n",
       "      <td>0.544280</td>\n",
       "      <td>0.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028792</td>\n",
       "      <td>0.154978</td>\n",
       "      <td>0.557800</td>\n",
       "      <td>0.369140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010508</td>\n",
       "      <td>0.117211</td>\n",
       "      <td>0.570940</td>\n",
       "      <td>0.369040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL_1_INDICATOR</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL_2_INDICATOR</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL_3_INDICATOR</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL_4_INDICATOR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OF_LIMIT_BAL_5_INDICATOR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051917</td>\n",
       "      <td>0.042562</td>\n",
       "      <td>0.232099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_2</th>\n",
       "      <td>0.222115</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.106937</td>\n",
       "      <td>0.041859</td>\n",
       "      <td>6.469312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372856</td>\n",
       "      <td>0.073752</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.279057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.305623</td>\n",
       "      <td>0.069779</td>\n",
       "      <td>0.038850</td>\n",
       "      <td>0.429799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066899</td>\n",
       "      <td>0.036914</td>\n",
       "      <td>0.035987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.613309</td>\n",
       "      <td>0.321564</td>\n",
       "      <td>0.033844</td>\n",
       "      <td>0.035492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_1_INDICATOR</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_2_INDICATOR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_3_INDICATOR</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_4_INDICATOR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_RATIO_5_INDICATOR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0          1          2          3  \\\n",
       "AGE                               24.000000  26.000000  34.000000  37.000000   \n",
       "PAY_1                              2.000000  -1.000000   0.000000   0.000000   \n",
       "PAY_2                              2.000000   2.000000   0.000000   0.000000   \n",
       "PAY_3                             -1.000000   0.000000   0.000000   0.000000   \n",
       "PAY_4                             -1.000000   0.000000   0.000000   0.000000   \n",
       "PAY_5                             -2.000000   0.000000   0.000000   0.000000   \n",
       "PAY_6                             -2.000000   2.000000   0.000000   0.000000   \n",
       "Y                                  1.000000   1.000000   0.000000   0.000000   \n",
       "SEX_Female                         1.000000   1.000000   1.000000   1.000000   \n",
       "SEX_Male                           0.000000   0.000000   0.000000   0.000000   \n",
       "EDUCATION_Graduate School          0.000000   0.000000   0.000000   0.000000   \n",
       "EDUCATION_Other                    0.000000   0.000000   0.000000   0.000000   \n",
       "EDUCATION_University               1.000000   1.000000   1.000000   1.000000   \n",
       "MARRIAGE_Married                   1.000000   0.000000   0.000000   1.000000   \n",
       "MARRIAGE_Non-married               0.000000   1.000000   1.000000   0.000000   \n",
       "PERCENT_OF_LIMIT_BAL1              0.195650   0.022350   0.308011   0.899800   \n",
       "PERCENT_OF_LIMIT_BAL2              0.120650   0.006042   0.139189   0.924280   \n",
       "PERCENT_OF_LIMIT_BAL3              0.034450   0.014017   0.139544   0.961820   \n",
       "PERCENT_OF_LIMIT_BAL4              0.000000   0.018933   0.148122   0.544280   \n",
       "PERCENT_OF_LIMIT_BAL5              0.000000   0.028792   0.154978   0.557800   \n",
       "PERCENT_OF_LIMIT_BAL6              0.000000   0.010508   0.117211   0.570940   \n",
       "PERCENT_OF_LIMIT_BAL_1_INDICATOR   1.000000   1.000000   1.000000   0.000000   \n",
       "PERCENT_OF_LIMIT_BAL_2_INDICATOR   1.000000   0.000000   0.000000   0.000000   \n",
       "PERCENT_OF_LIMIT_BAL_3_INDICATOR   1.000000   0.000000   0.000000   1.000000   \n",
       "PERCENT_OF_LIMIT_BAL_4_INDICATOR   0.000000   0.000000   0.000000   0.000000   \n",
       "PERCENT_OF_LIMIT_BAL_5_INDICATOR   0.000000   1.000000   1.000000   0.000000   \n",
       "PAY_RATIO_1                        0.000000   0.000000   0.051917   0.042562   \n",
       "PAY_RATIO_2                        0.222115   0.579710   0.106937   0.041859   \n",
       "PAY_RATIO_3                        0.000000   0.372856   0.073752   0.024345   \n",
       "PAY_RATIO_4                        1.000000   0.305623   0.069779   0.038850   \n",
       "PAY_RATIO_5                        1.000000   0.000000   0.066899   0.036914   \n",
       "PAY_RATIO_6                        1.000000   0.613309   0.321564   0.033844   \n",
       "PAY_RATIO_1_INDICATOR              1.000000   1.000000   1.000000   0.000000   \n",
       "PAY_RATIO_2_INDICATOR              0.000000   0.000000   0.000000   0.000000   \n",
       "PAY_RATIO_3_INDICATOR              0.600000   0.000000   0.000000   0.600000   \n",
       "PAY_RATIO_4_INDICATOR              0.000000   0.000000   0.000000   0.000000   \n",
       "PAY_RATIO_5_INDICATOR              0.000000   0.200000   0.200000   0.000000   \n",
       "\n",
       "                                          4  \n",
       "AGE                               57.000000  \n",
       "PAY_1                             -1.000000  \n",
       "PAY_2                              0.000000  \n",
       "PAY_3                             -1.000000  \n",
       "PAY_4                              0.000000  \n",
       "PAY_5                              0.000000  \n",
       "PAY_6                              0.000000  \n",
       "Y                                  0.000000  \n",
       "SEX_Female                         0.000000  \n",
       "SEX_Male                           1.000000  \n",
       "EDUCATION_Graduate School          0.000000  \n",
       "EDUCATION_Other                    0.000000  \n",
       "EDUCATION_University               1.000000  \n",
       "MARRIAGE_Married                   1.000000  \n",
       "MARRIAGE_Non-married               0.000000  \n",
       "PERCENT_OF_LIMIT_BAL1              0.132340  \n",
       "PERCENT_OF_LIMIT_BAL2             -0.620220  \n",
       "PERCENT_OF_LIMIT_BAL3              0.516700  \n",
       "PERCENT_OF_LIMIT_BAL4              0.238800  \n",
       "PERCENT_OF_LIMIT_BAL5              0.369140  \n",
       "PERCENT_OF_LIMIT_BAL6              0.369040  \n",
       "PERCENT_OF_LIMIT_BAL_1_INDICATOR   1.000000  \n",
       "PERCENT_OF_LIMIT_BAL_2_INDICATOR   0.000000  \n",
       "PERCENT_OF_LIMIT_BAL_3_INDICATOR   1.000000  \n",
       "PERCENT_OF_LIMIT_BAL_4_INDICATOR   0.000000  \n",
       "PERCENT_OF_LIMIT_BAL_5_INDICATOR   1.000000  \n",
       "PAY_RATIO_1                        0.232099  \n",
       "PAY_RATIO_2                        6.469312  \n",
       "PAY_RATIO_3                        0.279057  \n",
       "PAY_RATIO_4                        0.429799  \n",
       "PAY_RATIO_5                        0.035987  \n",
       "PAY_RATIO_6                        0.035492  \n",
       "PAY_RATIO_1_INDICATOR              1.000000  \n",
       "PAY_RATIO_2_INDICATOR              0.000000  \n",
       "PAY_RATIO_3_INDICATOR              0.600000  \n",
       "PAY_RATIO_4_INDICATOR              0.000000  \n",
       "PAY_RATIO_5_INDICATOR              0.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/Sequential_data_trimmed.csv')\n",
    "y = df['Y']\n",
    "\n",
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross-validation results: 0.6898666666666667\n"
     ]
    }
   ],
   "source": [
    "#make pipeline\n",
    "\n",
    "\n",
    "\n",
    "#split into training and testing\n",
    "Train_data, test_data = train_test_split(df, test_size = 0.2, random_state = 2019)\n",
    "\n",
    "#set the target and predictor labels\n",
    "target = 'Y'\n",
    "predictors = [x for x in df.columns if x not in [target] ]\n",
    "\n",
    "#build the classifier\n",
    "clf = LogisticRegression()\n",
    "\n",
    "#set up the imbalanced data handling procedure\n",
    "oversampler = SMOTE(random_state = 2019)\n",
    "\n",
    "#creat the pipeline\n",
    "pipeline = Pipeline([('smote', oversampler),('clf', clf)])\n",
    "\n",
    "#cross validate results\n",
    "kfold = KFold(n_splits=5, random_state=2019)\n",
    "results = cross_val_score(pipeline, df[predictors], df[target], cv=kfold, scoring = 'accuracy')\n",
    "print(f\"5-fold cross-validation results: {np.mean(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.696 \n",
      "\n",
      "AUC:0.673 \n",
      "\n",
      "F1:0.472 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.71      0.79      4710\n",
      "           1       0.38      0.63      0.47      1290\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      6000\n",
      "   macro avg       0.63      0.67      0.63      6000\n",
      "weighted avg       0.77      0.70      0.72      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#make helper function\n",
    "\n",
    "def evaluate_clf(data, model, imbalance='SMOTE', params=None, show_accuracy=True, show_auc=True, show_f1=True):\n",
    "    \n",
    "\n",
    "    #split into training and testing\n",
    "    Train_data, test_data = train_test_split(data, test_size = 0.2, random_state = 2019)\n",
    "\n",
    "    #set the target and predictor labels\n",
    "    target = 'Y'\n",
    "    predictors = [x for x in data.columns if x not in [target] ]\n",
    "\n",
    "    #build the classifier\n",
    "    clf = model\n",
    "\n",
    "    #set up the imbalanced data handling procedure\n",
    "    if imbalance=='SMOTE':\n",
    "        oversampler = SMOTE(random_state = 2019)\n",
    "    else:\n",
    "        oversampler = RandomOverSampler(random_state=2019)\n",
    "    #creat the pipeline\n",
    "    pipeline = Pipeline([('smote', oversampler),('clf', clf)])\n",
    "    pipeline.fit(Train_data[predictors], Train_data[target])\n",
    "    y_pred = pipeline.predict(test_data[predictors])\n",
    "    \n",
    "    if show_accuracy:\n",
    "        print (\"Accuracy:{0:.3f}\".format(accuracy_score(test_data[target],y_pred)),\"\\n\")\n",
    "        \n",
    "    if show_auc:\n",
    "        print (\"AUC:{0:.3f}\".format(roc_auc_score(test_data[target],y_pred)),\"\\n\")\n",
    "        \n",
    "    if show_f1:\n",
    "        print (\"F1:{0:.3f}\".format(f1_score(test_data[target],y_pred)),\"\\n\")\n",
    "    \n",
    "    print(classification_report(test_data[target], y_pred))\n",
    "\n",
    "evaluate_clf(df, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for pipeline\n",
    "def evaluate_pipeline(data, pipeline, params=None, show_accuracy=True, show_auc=True, show_f1=True):\n",
    "    \n",
    "\n",
    "    #split into training and testing\n",
    "    Train_data, test_data = train_test_split(data, test_size = 0.2, random_state = 2019)\n",
    "\n",
    "    #set the target and predictor labels\n",
    "    target = 'Y'\n",
    "    predictors = [x for x in data.columns if x not in [target] ]\n",
    "    \n",
    "    #fit the pipeline\n",
    "    pipe = pipeline\n",
    "    pipe.fit(Train_data[predictors], Train_data[target])\n",
    "    y_pred = pipeline.predict(test_data[predictors])\n",
    "    \n",
    "    if show_accuracy:\n",
    "        print (\"Accuracy:{0:.3f}\".format(accuracy_score(test_data[target],y_pred)),\"\\n\")\n",
    "        \n",
    "    if show_auc:\n",
    "        print (\"AUC:{0:.3f}\".format(roc_auc_score(test_data[target],y_pred)),\"\\n\")\n",
    "        \n",
    "    if show_f1:\n",
    "        print (\"F1:{0:.3f}\".format(f1_score(test_data[target],y_pred)),\"\\n\")\n",
    "    \n",
    "    print(classification_report(test_data[target], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:271: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 53.54 seconds for 10 candidates parameter settings.\n",
      "\n",
      "RandomizedSearchCV grid model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) with parameters {'penalty': 'l1', 'C': 1} had a best score of 0.6576142783769903\n"
     ]
    }
   ],
   "source": [
    "#create RandomSearchCV helper function\n",
    "def evaluate_random_grid(data, model, grid_params, score, n_iter=10, imbalance='SMOTE',\n",
    "                         cv = 5):\n",
    "\n",
    "    #set the target and predictor labels\n",
    "    target = 'Y'\n",
    "    predictors = [x for x in data.columns if x not in [target] ]\n",
    "\n",
    "    #build the classifier\n",
    "    grid = RandomizedSearchCV(estimator = model, param_distributions = grid_params, n_iter=n_iter, \n",
    "                            cv = cv, n_jobs=-1, scoring = score)\n",
    "\n",
    "    #set up the imbalanced data handling procedure\n",
    "    if imbalance=='SMOTE':\n",
    "        oversampler = SMOTE(random_state = 2019)\n",
    "    else:\n",
    "        oversampler = RandomOverSampler(random_state=2019)\n",
    "    \n",
    "    #apply class balancing\n",
    "    data_oversampled_X, data_oversampled_y = oversampler.fit_resample(data[predictors], data[target])\n",
    "    \n",
    "    #fit the random search\n",
    "    start = time()\n",
    "    cv_results = grid.fit(data_oversampled_X, data_oversampled_y)\n",
    "    print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter))\n",
    "    print()\n",
    "    \n",
    "    print(f\"RandomizedSearchCV grid model {model} with parameters {cv_results.best_params_} had a best score of {cv_results.best_score_}\")\n",
    "    \n",
    "\n",
    "clf = LogisticRegression()\n",
    "param_grid = {\n",
    "    'C': [1,5],\n",
    "    'penalty': ['l1', 'l2']\n",
    "             }\n",
    "evaluate_random_grid(df, clf, grid_params=param_grid, n_iter=10, score = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:271: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 32.38 seconds for 10 candidates parameter settings.\n",
      "\n",
      "RandomizedSearchCV grid model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) with parameters {'penalty': 'l1', 'C': 1} had a best score of 0.657678479712378\n"
     ]
    }
   ],
   "source": [
    "#create GridSearchCV helper\n",
    "def evaluate_gridsearch(data, model, grid_params, score, imbalance='SMOTE',\n",
    "                         cv = 5):\n",
    "\n",
    "    #set the target and predictor labels\n",
    "    target = 'Y'\n",
    "    predictors = [x for x in data.columns if x not in [target] ]\n",
    "\n",
    "    #build the classifier\n",
    "    grid = GridSearchCV(estimator = model, param_grid = grid_params, \n",
    "                            cv = cv, n_jobs=-1, scoring = score)\n",
    "\n",
    "    #set up the imbalanced data handling procedure\n",
    "    if imbalance=='SMOTE':\n",
    "        oversampler = SMOTE(random_state = 2019)\n",
    "    else:\n",
    "        oversampler = RandomOverSampler(random_state=2019)\n",
    "    \n",
    "    #apply class balancing\n",
    "    data_oversampled_X, data_oversampled_y = oversampler.fit_resample(data[predictors], data[target])\n",
    "    \n",
    "    #fit the random search\n",
    "    start = time()\n",
    "    cv_results = grid.fit(data_oversampled_X, data_oversampled_y)\n",
    "    print(\"GridSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter))\n",
    "    print()\n",
    "    \n",
    "    print(f\"GridSearchCV grid model {model} with parameters {cv_results.best_params_}  had a best score of {cv_results.best_score_}\")\n",
    "    \n",
    "    \n",
    "\n",
    "clf = LogisticRegression()\n",
    "param_grid = {\n",
    "    'C': [1,5],\n",
    "    'penalty': ['l1', 'l2']\n",
    "             }\n",
    "evaluate_random_grid(df, clf, grid_params=param_grid, n_iter=10, score = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.697 \n",
      "\n",
      "AUC:0.673 \n",
      "\n",
      "F1:0.472 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.71      0.79      4710\n",
      "           1       0.38      0.63      0.47      1290\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      6000\n",
      "   macro avg       0.63      0.67      0.63      6000\n",
      "weighted avg       0.77      0.70      0.72      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(df, LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.697 \n",
      "\n",
      "AUC:0.673 \n",
      "\n",
      "F1:0.473 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.71      0.79      4710\n",
      "           1       0.38      0.63      0.47      1290\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      6000\n",
      "   macro avg       0.63      0.67      0.63      6000\n",
      "weighted avg       0.77      0.70      0.72      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing a pipeline object in the helper functions\n",
    "clf = LogisticRegression()\n",
    "pipe = Pipeline([\n",
    "    ('smote', SMOTE(random_state=2019)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', clf)])\n",
    "\n",
    "evaluate_pipeline(data=df, pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:271: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 28.00 seconds for 10 candidates parameter settings.\n",
      "\n",
      "RandomizedSearchCV grid model Pipeline(memory=None,\n",
      "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))]) with parameters {'clf__penalty': 'l1', 'clf__C': 0.5} had a best score of 0.6576356788221195\n"
     ]
    }
   ],
   "source": [
    "#testing same code but with a random search\n",
    "clf = LogisticRegression()\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', clf)])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__C': [0.5,1, 1.5],\n",
    "    'clf__penalty': ['l1', 'l2']\n",
    "             }\n",
    "evaluate_random_grid(df, model=pipe, cv=5, grid_params=param_grid,score='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# This dataset is way to high-dimensional. Better do PCA:\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Maybe some original features where good, too?\n",
    "selection = SelectKBest(k=1)\n",
    "\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "# Model:\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Do grid search over k, n_components and C:\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n",
    "param_grid = dict(features__pca__n_components=[1, 2, 3],\n",
    "                  features__univ_select__k=[1, 2],\n",
    "                  svm__C=[0.1, 1, 10])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10, n_jobs=-1 )\n",
    "grid_search.fit(df[predictors], y)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.801 \n",
      "\n",
      "AUC:0.642 \n",
      "\n",
      "F1:0.440 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88      4710\n",
      "           1       0.56      0.36      0.44      1290\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6000\n",
      "   macro avg       0.70      0.64      0.66      6000\n",
      "weighted avg       0.78      0.80      0.78      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# attempting with a random forest\n",
    "\n",
    "clfRF = RandomForestClassifier()\n",
    "evaluate_clf(data=df, model=clfRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.6963333333333334\n"
     ]
    }
   ],
   "source": [
    "#testing the seperate fit to training, and predicting\n",
    "\n",
    "clf4 = LogisticRegression()\n",
    "\n",
    "pipeline4 = Pipeline([('smote', oversampler), ('clf', clf4)])\n",
    "pipeline4.fit(Train_data[predictors], Train_data[target])\n",
    "\n",
    "y_pred = pipeline4.predict(test_data[predictors])\n",
    "\n",
    "print(f\"accuracy score: {accuracy_score(test_data[target], y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross-validation results: 0.7969999999999999\n"
     ]
    }
   ],
   "source": [
    "#make pipeline with randomforest\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#split into training and testing\n",
    "Train_data, test_data = train_test_split(df, test_size = 0.2, random_state = 2019)\n",
    "\n",
    "target = 'Y'\n",
    "predictors = [x for x in Train_data.columns if x not in [target] ]\n",
    "\n",
    "clfRF = RandomForestClassifier()\n",
    "\n",
    "oversampler = SMOTE(random_state = 2019)\n",
    "pipeline = Pipeline([('smote', oversampler),('clf', clfRF)])\n",
    "\n",
    "#cross validate results\n",
    "kfold = KFold(n_splits=5, random_state=2019)\n",
    "results = cross_val_score(pipeline, Train_data[predictors], Train_data[target], cv=kfold, scoring = 'accuracy')\n",
    "print(f\"5-fold cross-validation results: {np.mean(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross-validation results: 0.7866666666666667\n"
     ]
    }
   ],
   "source": [
    "#compare the XGBoost models\n",
    "\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = joblib.load('Models/xgboost3.dat')\n",
    "#pipeline5 =  Pipeline([('smote', oversampler),('clf', model)])\n",
    "kfold = KFold(n_splits=5, random_state=2019)\n",
    "results = cross_val_score(model, test_data[predictors], test_data[target], cv=kfold, scoring = 'accuracy')\n",
    "print(f\"5-fold cross-validation results: {np.mean(results)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

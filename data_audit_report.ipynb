{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Audit\n",
    "\n",
    "This is 3-musketeers data audit report. We will discuss what we have done and what we plan to do.\n",
    "\n",
    "\n",
    "## Overall progress\n",
    "\n",
    "So far, we have made a pipeline as a base to compare future strategies against. It consists of some data manipulation and data transformations, as we will discuss below.\n",
    "\n",
    "As a note, all of the code can be found in `Kevin/data_pipeline.ipynb` notebook.\n",
    "\n",
    "The first thing we did was to read in the data into a suitable format. This involved reading the Excel file and naming the columns into something more intuitive than the original column names.\n",
    "\n",
    "There are no missing data points, so imputation is not necessary in this pipeline. \n",
    "\n",
    "Next, after noting the discrepencies in the values for the classes for  'EDUCATION', 'SEX', and 'MARRIAGE' features, we dediced to rename and replace many of these values. For instance, the 'EDUCATION' feature had a majority of  \"Graduate School\" and \"University\" class, and a very limited amount of the other classes, so we dicided to keep the 2 former classes and bin the latter classes into a category of \"Other\". A similar process was done for the other two features. Code for this is found in cell number 4 in [`base_pipeline.ipynb`]('Kevin/base_pipeline.ipynb') notebook.\n",
    "\n",
    "Then we decided to clean the quantitative featues of 'LIMIT_BAL', 'BILL_AMT1 : PAY_AMT6'. After displaying box-plots and histograms to show the distribution, we then used sklearn QuantileTransformer object to transform those features into a Gaussian shaped distribution. We then display the distibution after the transformations. Code for this step is in cell number 9.\n",
    "\n",
    "After that, we diecided to scale our data into a range of of 0 to 1 using MinMaxScaler; many of our features severe scaling differences, so we simply scaled them all on the same scale. \n",
    "\n",
    "The last step was to build a baseline model. In addition, we needed to fix the imbalance of the target feature. We decided to use the oversampling technique to remedy the imbalance, and then we build a simple Naive Bayes model and a simple Logistic Regression model. We tested a 10-fold cross validation score for each; our Naive Bayes does slightly better with a cross validation accuracy score of 67%. Since this value is pretty low, we can only go up from here!\n",
    "\n",
    "We explored some other strategies initially as well. Stuart tried to create new features based off his intuition and tested whether these features helped a baseline model. His results were inconclusive, so we may explore further into our experimentation. \n",
    "\n",
    "Martin is also applying different stratagies in the pipeline to see if they impact our results as well. \n",
    "\n",
    "### To do\n",
    "\n",
    "We will continue to test different strategies in our pipelines, such as different transformation techniques of our quantitative data, and potential binning strategies. Also, we will explore feature selection methods and possible interaction features, as Stuart was initially exploring. Of course, we will also try out different models to try and we learn them in class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
